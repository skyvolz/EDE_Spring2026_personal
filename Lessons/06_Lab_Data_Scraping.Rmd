---
title: "06: Lab - Data Scraping"
author: "Environmental Data Analytics | John Fay"
date: "Spring 2026"
editor_options: 
  chunk_output_type: console
---

## Objectives
1. Answer questions on M6
2. Web services: URLs, "REST", and APIs
3. Scraping Exercises

## Set up
```{r, message = FALSE}
#Install familiar packages
library(tidyverse);library(viridis);library(here)
here()

install.packages("rvest")
library(rvest)

#install.packages("dataRetrieval")
library(dataRetrieval)

#install.packages("tidycensus")
library(tidycensus)

# Set theme
mytheme <- theme_gray() +
  theme(axis.text = element_text(color = "black"), 
        legend.position = "top")
theme_set(mytheme)
```

## Web Services 

### An Exercise
* Open your browser to Google's home page
* Search for "Duke University"
* Check the URL of the search result
 - Within the url is `q=Duke+University`
* Try the URL: "https://www.google.com/search?q=NSOE"

### An Explainer
* Interacting with the web boils down to **requests** and **responses**

* Requests are often sent as browser URLs. They usually include:
 - The address of the server that will handle the request
 - The name of the service on that server that will process the request
 - The parameters of the service
 > Can you identify these components in the Google search request? 
 
* Responses are sent as text files, which are interpreted by a browser
 - Written in HTML: Tags (w/attributes) & Values, which browsers can interpret
 - Often include JavaScript, which browsers can run
 
* "REST" is the framework of sending requests and handling responses - all in text.
 - "REpresentational State Transfer"
 - REST is a form of an API..
 
### Why do we care? Because we can exploit this to fetch data! 

* Navigate to: <https://waterdata.usgs.gov/>
* Select **Go to the National Water Dashboard**
  - <https://dashboard.waterdata.usgs.gov/app/nwd/en/>
* Search for the site number `02087500` and select the result "Neuse River Near Clayton, NC"
* Click on the point, a hydrograph will open
  - <https://dashboard.waterdata.usgs.gov/api/gwis/2.1.1/service/site?agencyCode=USGS&siteNumber=02087500&open=89192>
* Click on the `site page` link
  - <https://waterdata.usgs.gov/monitoring-location/USGS-02087500>
* Open the browser's "Developer's Tools Interface" (<ctrl>-<shift>-<i>)
  - Activate the "Network" tab
  - Reload the page
  - Filter for "Fetch/XHR"
  - Sort on size
  - Browse for URLs that look like requests
  - Find and test some URLS:
  
  > <https://api.waterdata.usgs.gov/ogcapi/v0/collections/field-measurements/items?limit=10000&skipGeometry=true&properties=parameter_code,value,unit_of_measure,time,qualifier,approval_status&monitoring_location_id=USGS-02087500>
  
#### Examine the API documentation: 
<https://api.waterdata.usgs.gov/ogcapi/v0/openapi?f=html#/continuous>
* Replace `time_series_id=...` with `monitoring_location_id=USGS-02087500`
* Add "%f=csv" to end

```{r}
#Create the URL
url=paste0(
  "https://api.waterdata.usgs.gov/ogcapi/v0/collections/continuous/items?",
  "limit=50000&",
  "properties=time,parameter_code,value,unit_of_measure,approval_status,qualifier&",
  #"time_series_id=438fae403f374eb680a050a9c3a67148&",
  "monitoring_location_id=USGS-02087500&",
  "time=P7D&",
  "f=csv")

#Read the URL
water_data_raw <- read.csv(url)  
#Wrangle and plot
water_data <- water_data_raw %>% 
  select(-x,-y) %>% 
  pivot_wider(
    names_from=parameter_code,
    values_from=value,
    id_cols = time
  ) %>% 
  mutate(time=ymd_hms(time)) 

#Update names
names(water_data) = c('date','unk','discharge_cfs','gageht_ft')

#Plot
water_data %>% 
  ggplot(aes(x=date,y=discharge_cfs)) + 
  geom_line() 

```


### Exercise: 
> The above code chunk is copied below. Edit it so that it plots the last *24* days of flow for gage *02085070*

```{r EXERCISE - pull.and.plot.other.discharge.data}
#Create the URL
url=paste0(
  "https://api.waterdata.usgs.gov/ogcapi/v0/collections/continuous/items?",
  "limit=50000&",
  "properties=time,parameter_code,value,unit_of_measure,approval_status,qualifier&",
  #"time_series_id=438fae403f374eb680a050a9c3a67148&",
  "monitoring_location_id=USGS-02085070&", #change site number
  "time=P24D&", #change number of days
  "f=csv")

#Read the URL
water_data_raw <- read.csv(url)  
#Wrangle and plot
water_data <- water_data_raw %>% 
  select(-x,-y) %>% 
  pivot_wider(
    names_from=parameter_code,
    values_from=value,
    id_cols = time
  ) %>% 
  mutate(time=ymd_hms(time)) 

#Update names
names(water_data) = c('date','unk','discharge_cfs','gageht_ft')

#Plot
water_data %>% 
  ggplot(aes(x=date,y=discharge_cfs)) + 
  geom_line() 
```


## APIs

Application programming interfaces, or APIs, are a formalization of the REST request/response process. More precisely, they are web sites that have been designed to allow a client to query specific bits of data from an on-line resource without just "hacking" a URL.

More and more organizations are providing API access to their data. Have a look at the US Census catalog of APIs to grab demographic data: <https://www.census.gov/data/developers/data-sets.html>.

And with the formalization of APIs, more R packages are being written to streamline access to data via these APIs. Let's look at some examples. 


### Grabing discharge data via  the `dataRetrieval` package
More info: <https://cran.r-project.org/web/packages/dataRetrieval/vignettes/dataRetrieval.html>

```{r using.the.dataRetrieval.package}
#Identify gauge to download
siteNo <- 'USGS-02087500' 

#Identify parameter of interest: 
pcode = '00060' #discharge (cfs)

#Identify statistic code for daily values: 
scode = "00003"  #mean

#Identify start and end dates
start.date = "1930-10-01"
end.date = Sys.Date() #Gets today's date

#Load in the data uing the package's "read_waterdata_daily" function
neuse <- read_waterdata_daily(
  monitoring_location_id = siteNo,
  parameter_code = pcode,
  time = c(start.date,end.date),
  skipGeometry = TRUE
)

  
#Compute annual means
neuse_by_year <- neuse %>% 
  group_by(year = year(time)) %>% 
  summarise(mean_annual_flow = mean(value))

#Plot
ggplot(neuse_by_year, aes(x=year,y=mean_annual_flow)) + 
  geom_line() +
  labs(y="Mean Annual Flow (cfs)")
```

### Using `tidycensus` to extract census data
The `tidycensus` package uses the Census' API to pull data into our coding environment. To use it, however, you first need to get a free Census API key at <https://api.census.gov/data/key_signup.html>. After that, it's pretty easy to fetch data into your coding environment. 

We'll use it to extract total population of counties in NC. 

You'll also need to look up the IDs of the variables you want to extract. Those are found here:
<https://walker-data.com/tidycensus/articles/basic-usage.html#searching-for-variables>


```{r}
#Apply the API key
#census_api_key("d4ad7874c996c7a83fec0a81466309ddc07d8e22a", install = TRUE)

nc_pop <- 
  get_acs(geography = "county",
          variables = "B01003_001",
          state = "NC",
          geometry = TRUE)

ggplot(nc_pop, aes(fill = estimate)) +
  geom_sf() +
  coord_sf(crs = 4326) +
  scale_fill_viridis(option = "viridis")

```

## Scraping Exercises
APIs are great, when they are available. But if not, we can still scrape data from a web site. 

Let's scrape data the US Energy Administration's State Electricity Archive:
<https://www.eia.gov/electricity/state/>

1. Navigate to the page. Note the year for which these data are provided.
2. Use the Selector Gadget tool to scrape the following values into lists: 
 * State name
 * Average Retail price
 * Net summer capacity
 * Net generation
 * Total retail sales
3. Compile the data into a table
4. Repeat for 2021 

The RVest scraping workflow is as follows: 
1. Connect to the website using the `read_html` function.
2. Locate specific elements in the web site via the node IDs found using Selector Gadget, reading them in using `html_nodes`
3. Read the text value(s) associated with those nodes into the coding environment via `html_text`
4. Wrangle values into a dataframe, being sure to convert numeric values to numbers

```{r EXERCISE - scrape.data.manually eval=FALSE}
#1 Link to the web site using read_html
the_website <- read_html("https://www.eia.gov/electricity/state/")

#2&3 Locate elements and read their text attributes into variables
the_states <- the_website %>% html_nodes("td:nth-child(1)") %>% html_text()
the_price <- the_website %>% html_nodes("td:nth-child(2)") %>% html_text()
the_capacity <- the_website %>% html_nodes("td:nth-child(3)") %>% html_text()
net_generation <- the_website %>% html_nodes("td:nth-child(4)") %>% html_text()
total_retail <- the_website %>% html_nodes("td:nth-child(5)") %>% html_text()
  
#3 Construct a dataframe from the values
the_df <- data.frame(
  state = the_states,
  price = as.numeric(the_price),
  capacity = as.numeric(gsub(",","", net_generation)),
  total_retail = as.numeric(gsub(",","", total_retail)) 
) %>% 
  filter(state != "U.S. Total") %>% 
  mutate(year = the_year)


#gsub(",","",my_num)
#as.numeric((",","",my_num))

```


```{r create.scrape.function}
scrape.it <- function(the_year){
  the_url <- ifelse(
    the_year>='2023',
    'https://www.eia.gov/electricity/state/',
    paste0('https://www.eia.gov/electricity/state/archive/',the_year,'/')
    )

  #Fetch the website
  #<enter code from above here> 
  the_website <- read_html(the_url)
 
   #Scrape the data
  #<enter code from above here>
  the_states <- the_website %>% html_nodes("td:nth-child(1)") %>% html_text()
  
  #Convert to dataframe
  #<enter code from above here, adding a column for the_year>
the_df <- data.Fram(states = the_states)

  #Return the dataframe
  return(the_df)
}

# Use the above function to scrape data for 2017
df2017 <- scrape.it(2017)

# Map the function to scrape data from 2021 to 2023
df_range <- map(c(2017:2024),scrape.it) %>% bind_rows()

#some plots
df_range %>% 
  filter(state %in% c('Virginia', 'North Carolina', 'South Carolina')) %>% 
  ggplot(aes(x=mdy(paste0("1-1",year)), y=price, color=state)) +
  geom_smoot(method='loess', se=FALSE) +
  scale_x_date("Year", breaks='1 year', date_labels = '%Y') +
  theme(legend.position = 'top')
```